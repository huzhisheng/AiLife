> Agent = 模型
>
> State, Observation = 环境的状态
>
> Action = 改变环境
>
> Reward = 环境给Agent的反馈

> 强化学习的问题就是通常的reward都是0，只有少数的action会有非0的reward
>
> 如果按照supervised learning的方式来解决RL的问题，最大的问题就是机器不知道学习什么重要而什么不重要（例如一张红绿灯图中有个鸟，可能机器就会认为看见鸟时就踩刹车）
>
> PS：AlphaGO是先supervised learning，然后再RL

> Episode：从初始状态一直到结束状态，这样一趟叫一个episode



Difficulties：

1. 如何让Agent不只追求眼前reward

2. 如何让Agent尝试探索其它方式（不局限于之前的操作）



Outline：

1. Policy-based，Actor
2. Value-based，Critic

# Policy-based Approach

### PBA：Learning an Actor

> 本节中我们用$\pi$表示function的意思
>
> Actor实际就是$\pi$，就是个function

$$
Action=\pi(Observation)
$$

Step1：选择模型
在这部分假设Actor就是个NN

Step2：定义模型好坏
将每一个step的reward加起来，最后的total reward可以作为模型好坏$R_\theta$（$\theta$是代表模型中的参数），但我们并不是希望只让某一次游戏玩得好，因为游戏有随机性，我们的模型在做Action时也有随机性（假设NN输出的是采取各个动作的概率），因此我们希望的是让期望值$\overline R_\theta$越大越好

其中期望值$\overline R_\theta$的计算公式如下：

- 假设$\tau$是一场游戏过程，s是state，a是action，r是reward
  则$\tau=\lbrace s_1,a_1,r_1,s_2,a_2,r_2,\ \dots,s_T,a_T,r_T\rbrace$
- $R(\tau)=\sum_{n=1}^Tr_n$

- 假设对于模型的参数$\theta$某个过程$\tau$出现的概率为$P(\tau|\theta)$，则
  $\overline R_\theta=\sum_\tau R(\tau)P(\tau|\theta)$
  
  实际中$P(\tau|\theta)$就是让$\pi_{\theta}$去玩N场游戏，然后根据某个$\tau$的出现次数来算$P(\tau|\theta)$，则
  
  $\overline R_\theta=\sum_\tau R(\tau)P(\tau|\theta)=\frac 1N\sum_{n=1}^NR(\tau^n)$

Step3：训练模型
仍然是gradient descent，$\theta^1 \leftarrow \theta^0+\eta \nabla \overline R_{\theta^0}$，且计算R的微分不需要知道$R(\tau)$
$$
\begin{split}
\nabla\overline R_{\theta}&=\sum_\tau R(\tau)\nabla P(\tau|\theta) \\
&=\sum_\tau R(\tau)P(\tau|\theta)\frac {\nabla P(\tau|\theta)}{P(\tau|\theta)}  \\
&=\sum_\tau R(\tau)P(\tau|\theta)\nabla logP(\tau|\theta) \\
&\approx \frac 1N\sum_{n=1}^NR(\tau^n)\nabla log P(\tau^n|\theta)
\end{split}
$$
而
$$
\begin{split}
P(\tau|\theta)&=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)\cdots\\
&=p(s_1)\prod_{t=1}^T p(a_t|s_t,\theta)p(r_t,s_{t+1}|s_t,a_t)
\end{split}
$$
于是
$$
logP(\tau|\theta)=logp(s_1)+\sum_{t=1}^Tlogp(a_t|s_t,\theta)+logp(r_t,s_{t+1}|s_t,a_t)
$$

因此

$$
\nabla logP(\tau|\theta)=\sum_{t=1}^T \nabla logp(a_t|s_t,\theta)
$$

于是乎（回顾：$\nabla$是代表将后面的式子对$\theta$求微分）
$$
\begin{split}
\nabla\overline R_{\theta}&\approx \frac 1N\sum_{n=1}^NR(\tau^n)\nabla log P(\tau^n|\theta) \\
&=\frac 1N \sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla logp(a^n_t|s^n_t,\theta)
\end{split}
$$
就是希望如果某次$R(\tau)$为正，则希望调整$\theta$使得其中的$p(a^n_t|s^n_t)$都变大，否则都变小。



**其中的问题**
因为我们的在各个step中采取的动作可能有多种都是正reward，但是如果其它那些没有被sample到，就会使得那些没被sample到的action在对应的环境中被采取的概率下降（因为采用softmax）。为了改善这个问题，可以加个bias，如下
$$
\begin{split}
\nabla\overline R_{\theta}&\approx \frac 1N \sum_{n=1}^N\sum_{t=1}^{T_n}(R(\tau^n)-b)\nabla logp(a^n_t|s^n_t,\theta)
\end{split}
$$
即$R(\tau)$大于一定bias后才可进行优化，否则是减少其中的p的概率。





# Value-based Approach

### VBA：Learning a Critic

Critic并没有告诉每一个step怎么走，只是给它一个$\pi$，然后Critic会对这个actor进行打分。具体是，actor会告诉某个**actor**在当前**state**的情况的期望未来reward值总和
$$
V^{\pi}(s)
$$
如何去估计$V^{\pi}(s)$？

1. **MC**，用**蒙特卡罗模拟法**，就是实际地用actor去玩游戏看其reward值（一直玩直到游戏结束，多次玩求均值）。然后现在这个critic就是要让当前的$V^\pi$在输入当前的s后的输出尽量和真实游玩的reward值越接近越好。

2. **TD**，**时序差分法**，例如观察到的数据是{$\cdots,s_t,a_t,r_t,s_{t+1},\cdots$}，那么就要求st和st+1的期望值要相差rt
   $$
   V^\pi(s_t)-V^\pi(s_{t+1})\approx r_t
   $$

### Another Critic：Q-Learning

$Q^\pi(s,a)$，意思是在当前actor $\pi$、环境为s、采取动作a的情况下，到游戏结束时预计会得到多少reward。

### Q-Learning

Q-Learning的步骤就是，在$\pi_t$的情况下，通过模拟法让这个actor去玩游戏，观察各个过程$\tau$和最后的reward值$R(\tau)$，训练出此时的$Q^{\pi_t}(s,a)$，有了这个Q后就可以去找到下一个actor $\pi_{t+1}$，然后就这么不断循环下去...

**问题1**：怎么根据这个Q，找到下一个更好的actor $\pi'$?
（PS：更好的定义是说，$V^{\pi'}\ge V^{\pi},for\ any\ state\ s$）
答案就是
$$
\pi'(s)=arg\ \mathop {max}_a Q^\pi(s,a)
$$
也就是说新的actor就是原来的actor的基础之上全部听Q的版本。（**知道了旧actor的弱点并能进行改善的，就是新actor**）

**问题2**：怎么求解上面的argmax问题?
离散的，用穷举法；（对于连续的action（无穷多种可能性action），暂时没讲）

对于连续的action，老师说是另外去学习一个actor $\pi$，这个actor的目的就是给定一个s，输出动作a，使得$Q^\pi(s,a)$最大，因此这个思想又是和GAN差不多的。（了解即可）



**问题3**：DQN的tip
阅读rainbow论文。







# Actor+Critic Methods：A3C

Asynchronous Advantaged Actor-Critic

之前的方法因为过程都是随机的，直接跟着随机的过程学效果就可能会不太好。因此新的方法的思想就是让actor不跟着环境的reward去学习，只需跟着critic学习。

A3C的Asynchronous的思想：
和火影忍者类似，有一个global的actor和critic，每次将global的actor和critic分身（复制多份参数），让子actor去和实际环境交互，得到新critic，并告诉原来的global该如何去更新。



# Inverse RL(IRL)

> 生活中很多场景是没有明确reward规则的。（例如自动驾驶）

Inverse RL是说，我们有专家的游戏记录{$\tau_1,\tau_2,\dots ,\tau_N$}，没有reward规则，想让actor尽量模仿专家。

Inverse RL的大致是根据专家的游戏记录推出reward function，然后再用RL的方法去找出最好的actor。

Inverse RL的步骤

1. 随机一个actor

2. 循环

   1. 当前actor去玩游戏，得到许多游戏记录

   2. 学习reward function，要求当前actor的游戏记录得分要比专家游戏记录得分低
   3. 根据新reward function去学习出下一个actor

> 而这个过程又又又和GAN差不多了

