> 不要用Regression的方法来试图解决Classification

## 分类问题介绍

将每个分类C看作是一个装着黑球和白球的箱子，每个箱子中黑球和白球的比例不同，而分类问题就是要求“当取出的球是x颜色时，推测它的类别”。

由贝叶斯定理可知（假设是二元分类C1和C2）：
$$
P(C_1|x)=\frac {P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}
$$
当然其实知道了$P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$的话就得到了P(x)，即得到了样本的distribution，此时我们就可以自己生成x。（generative model）



在李老师的课中，把P(x|C)看作是一种高斯分布，即不同种类C的x分布P(x|C)不同。

而又有向量的高斯分布概率密度公式如下：
$$
f_{\mu,\Sigma}(x)={\frac 1{(2\pi)^{D/2}} \frac {1}{|\Sigma|^{\frac 12}}} exp\lbrace -\frac 12(x-\mu)^T\Sigma^{-1}(x-\mu)\rbrace
$$
其中$\mu$是均值，$\Sigma$是协方差矩阵（相当于是变量的方差）

而对于某一个分布$f$，其产生各种样本的概率为（就是把概率密度乘起来）：
$$
L(\mu,\Sigma)=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2) \dots
$$
我们的目标就是找到这个高斯分布$\mu^*,\Sigma^*$使得$L$最大

> 用数学的方式解

$$
\mu^*=\frac 1{79}\sum^{79}_{n=1}x^n \\
\Sigma^*=\frac 1{79}\sum^{79}_{n=1}(x^n-\mu^*)(x^n-\mu^*)^T
$$

通过求出$f$我们便相当于拿到了$P(x|C)$，而$P(C)$则只需根据C种类个数除以总数即可获得，至此我们可以计算出$P(C|x)$

为每个种类C都维持一个协方差矩阵$\Sigma$的代价是比较大的，因此李老师在这里说可以让多个种类的$\Sigma$相同，计算公式如下（即把之前的$\Sigma$加权求和）：
$$
\Sigma=\frac {n1}{N}\Sigma^1+\frac {n2}{N}\Sigma^2
$$
共用$\Sigma$会变为Linear Model，即分界线是一条直线而非曲线。



上述的讨论中假设样本分布是一个向量高斯分布，但实际在训练时建模型可以假设成各种各样的分布模型，例如假设每个属性之间是独立分布的，那么就可以把向量高斯分布简化成多个一维高斯分布的乘积（**Naive贝叶斯**），这样模型会更简单，但是实际效果就可能会更差。


$$
\begin{split}
P(C_1|x)&=\frac {P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)} \\
&=\frac 1{1+\frac {P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}
\end{split}
$$
其中P(C1|x)就变得跟sigmoid函数相同形式
$$
\begin{split}
&\sigma(z)=\frac 1{1+exp(-z)} \\
&\begin{split}z&=ln\frac {P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)} \\
&=ln\frac {P(x|C_1)}{P(x|C_2)}+ln\frac {P(C_1)}{P(C_2)}
\end{split}
\end{split}
$$
将$P(x|C)$和$P(C)$都展开，将和$x$有关的放在一起，将和$x$无关的放在一起，并假设各个分类的分布协方差矩阵$\Sigma$共用的条件下，最后就可以化为
$$
P(C_1|x)=\sigma(z)=\sigma(w\cdot x+b)
$$
**也就是神经网络最初的模样**。

也就是说，给出样本，我们就可以算出$w,b$，接着给出一个$x$我们就可以算出它属于某个分类$C$的概率$P(C|x)$

但实际上我们并不需要麻烦地计算出$w,b$，而是只需要用神经网络去训练出一个近似的$w,b$就可以！

